{"task_name": "demo","code": "# coding=utf8\n\n\"\"\"\nthis is a demo project for a simply spider\n\n该平台的爬虫文件，在节点接收到任务后\nTaskReader 生成 ./spider/xxx/目录\n在 ./spider/xxx/目录下 生成 xxxSpider.py 脚本\n\n程序自动生成 xxx_seed.py、xxx_downloader.py、xxx_parser.py、xxx_persistence.py\n调用xxxSpider.py脚本里对于的模块，然后持续执行\n\"\"\"\n\n\n# 各类库导入部分\n# 正常的爬虫导入\n\nimport requests\n# from xxx import xxx\n# from yyy import zzz\n\nclass SeedsMaker():\n    \"\"\"种子生成器\n\n    作用就是提供一个逻辑，去生产种子\n    \n    向消息中心源源不断的丢种子进去\n    \"\"\"\n    \n    def fun1(self):\n        pass\n\n    def fun2(self):\n        pass\n    \n    def execute(self):\n        \"\"\"固定的调用函数\"\"\"\n        print('执行')\n\n\nclass Downloader():\n    \"\"\"下载器\n\n    其作用就是根据url，执行一次请求\n    \n    至于 headers\n    cookie啊\n    什么的，都写在这个模块里\n    \"\"\"\n\n    # from HTTP.RequestServerApi import ReqestApi\n    def fun1(self):\n        pass\n    \n    def fun2(self):\n        pass\n    \n    def execute(self):\n        \"\"\"固定的调用函数\"\"\"\n        pass\n\n\nclass Parser():\n    \"\"\"解析器\n\n    接收html，然后解析\n    当然在该Parser里，要承担的是好几个页面的解析\n    因为需要一个 switcher\n    \"\"\"\n\n    def parse1(self):\n        pass\n    \n    def parse2(self):\n        pass\n    \n    def switch(self):\n        pass\n    \n    def execute(self):\n        pass\n    \n\nclass Persistence():\n    \"\"\"持久化模块\n\n    根据要求，将相应的数据放入 hdfs、mysql、local里\n\n    \"\"\"\n\n    def execute(self):\n        pass\n    ", "quote": null}