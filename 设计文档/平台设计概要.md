# 抓取平台概要设计


|时间|撰写人|修订内容|备注|
|:-:|:---:|:-----:|:-:|
|2018.08.09|王家葳|建立文档|初稿|

## 需求

随着爬虫脚本的逐日增多，维护与管理的成本增加

抓取平台应运而生，承担调度、管理、维护的功能

## 抓取流程梗概

![抓取流程梗概][1]

## 系统组织结构:

- Scheduler: 调度器
- Crawler: 抓取器
- Parser: 解析器
- Persistence: 持久化

## 模块划分

- **调度模块**

![Scheduler][2]

- **抓取模块**

![Crawler][3]

- **解析模块/持久化模块**

![Parser&Persistence][4]

- **爬虫项目构成**

![CrawlerProject.py][5]

## 功能划分

- Scheduler: 调度器，承担解析项目，定时调度的功能

	- JobState: 完成种子的构造、url去重、定时执行等
	- ResourceState: 状态监测
	- logging: 记录日志
	- middleware: 任务分发
	
- Crawler: 抓取器，多个，承担接收种子，完成请求过程
	
	- SessionPool: session池，cookie管理
	- HttpApiPool: 请求池,根据条件完成该次请求
	- ProxyPool: 代理池，提供代理服务
	- UAPool: user-agent池，为爬虫提供不同的ua伪装各种身份
	- middleware: 中间件，接收种子，发送新url
	- logging: 日志模块
	 
- Parser& Persistence: 解析器，承担解析返回的HTML/JS并交给Persistence做持久化处理
	
	- API: 外部接口，调用项目中对应的解析及存储规则
	- middleware: 中间件，发送新url
	- logging: 日志模块
	

## 技术选型

- 调度部分：自己造轮子，参考pyspider的schedule
- 各爬取节点: 由supervisor负责启动和监控各爬取节点状态
- 请求部分：
	- http/1: 自己封装的HttpRequestApi
	- http/2: hyper
	- urllib
- 中间件部分：Redis
- 解析部分: lxml, json, re
- 存储部分: 本地I/O, MySql, hdfs

## 可量化目标阶段

- 第一阶段:设计阶段
	- 设计各个模块
- 第二阶段:各模块开发阶段
	- 开发各个模块
- 第三阶段:完成平台的原型开发
	- 暂不引入消息队列，各模块间通过文件通信
- 第四阶段:引入消息队列
	- 调通平台
- 优化阶段:根据积累的问题,逐个优化

## 出错处理
[1]:https://github.com/beforeuwait/spider_platform/blob/master/%E8%AE%BE%E8%AE%A1%E6%96%87%E6%A1%A3/%E6%8A%93%E5%8F%96%E5%B9%B3%E5%8F%B0%E6%80%BB%E8%A7%88.jpg?raw=true
[2]:https://github.com/beforeuwait/spider_platform/blob/master/%E8%AE%BE%E8%AE%A1%E6%96%87%E6%A1%A3/Scheduler%E6%9E%B6%E6%9E%84%E5%9B%BE.jpg?raw=true
[3]:https://github.com/beforeuwait/spider_platform/blob/master/%E8%AE%BE%E8%AE%A1%E6%96%87%E6%A1%A3/Crawler%E6%9E%B6%E6%9E%84%E5%9B%BE.jpg?raw=true
[4]:https://github.com/beforeuwait/spider_platform/blob/master/%E8%AE%BE%E8%AE%A1%E6%96%87%E6%A1%A3/%E8%A7%A3%E6%9E%90%E5%8F%8A%E5%AD%98%E5%82%A8%E6%A8%A1%E5%9D%97.jpg?raw=true
[5]:https://github.com/beforeuwait/spider_platform/blob/master/%E8%AE%BE%E8%AE%A1%E6%96%87%E6%A1%A3/%E7%88%AC%E8%99%AB%E9%A1%B9%E7%9B%AE%E7%BB%93%E6%9E%84.jpg?raw=true